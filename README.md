In this project the performance of two digit recognition models are compared statistically. The first model is contains 2 linear layers, the second model contains 2 linear layers of which one is a DropConnectLayer. A DropConnection is a layer that promotes generalization. The results of the performances for the two models are somewhat similar (around 97.23 and 97.91), but the statistical test shows the results should be considered different with a confidence interval of 95% and 99%. The normal distribution approximations for both performance results are also plotted in two histograms. These histograms have a very small overlap in their extreme tails, so when assuming the mean and standard deviation for these samples are representative for all respective samples, be it of linear or partly linear origin, than it is next to impossible to produce two samples (containing 10000 individual tests) that have their means closer to the overlapping tails so that 5% or 1%  of the individual test could belong to the other sample.
![norm_binom](https://github.com/user-attachments/assets/e48d5919-5723-4156-acf9-4aa155b9ebdc)
